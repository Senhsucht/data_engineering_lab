# ğŸ PySpark Exercises

Esta carpeta contiene ejercicios prÃ¡cticos de **PySpark**, diseÃ±ados para reforzar el uso de Spark en procesamiento de datos distribuidos.  
Los ejercicios estÃ¡n enfocados en operaciones comunes de **Data Engineering**, como limpieza, transformaciones, joins y agregaciones.

---

## ğŸ“‚ Estructura
Se generar una carpeta por proyecto cada uno tendra:

â”œâ”€â”€ data/                       # datasets de entrada
â”œâ”€â”€ notebooks/                  # versiÃ³n en notebook
â”œâ”€â”€ src/                        # cÃ³digo PySpark modularizado
â”œâ”€â”€ output/                     # aquÃ­ se guardan los Parquet generados
â””â”€â”€ README.md                   # explicaciÃ³n del proyecto


---

## ğŸ¯ Objetivos de la carpeta

- Familiarizarse con la **API de PySpark**.  
- Practicar la creaciÃ³n de **pipelines de transformaciÃ³n de datos**.  
- Reforzar conceptos clave:
  - Lectura y escritura de datos (CSV, Parquet, Delta).
  - Transformaciones (`select`, `withColumn`, `filter`, `drop`).
  - Joins (`inner`, `left`, `right`, `outer`).
  - Funciones de agregaciÃ³n y **Window functions**.
  - Manejo de errores y buenas prÃ¡cticas en scripts PySpark.

---

## âš¡ EjecuciÃ³n

1. Inicia sesiÃ³n en tu entorno PySpark (local o Databricks).  
   Ejemplo en local:

```bash
   pyspark
```

o usando spark-submit:

```bash
spark-submit ejercicio_01_basico.py
```

2. Revisa la salida en consola o los archivos generados en output/.